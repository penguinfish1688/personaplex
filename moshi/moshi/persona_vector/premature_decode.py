"""Premature decode utilities for inspecting layer-wise text logits.

Expected input: `output_hidden.pt` generated by `fdb.py --save-hidden`.

Payload schema (saved by `run_batch_inference(..., save_hidden_payload=True)`):
    - `text_hidden_layers`: torch.FloatTensor[T, L, D]
        T: generated token count (from input.wav processing only)
        L: number of main text transformer layers
        D: hidden size
    - `text_attention_weights`: list[torch.FloatTensor[L, H, K_t]]
        H: attention heads
        K_t: causal key length at token t (can vary)
    - `token_ids`: torch.LongTensor[T]
    - `token_names`: list[str]
    - `times`: torch.FloatTensor[T]
    - `token_time_ranges_sec`: torch.FloatTensor[T, 2]
    - `frame_rate`: float (12.5 Hz)

Token-time alignment:
    token 0 -> [0, 1/12.5) seconds
    token t -> [t/12.5, (t+1)/12.5) seconds
"""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Sequence

import matplotlib.pyplot as plt
import numpy as np
import sentencepiece
import torch
import torch.nn.functional as F
from huggingface_hub import hf_hub_download
from matplotlib.colors import LogNorm
from matplotlib.patches import Rectangle

from moshi.models import loaders


SPECIAL_TOKEN_MAP = {
    0: "EPAD",
    1: "BOS",
    2: "EOS",
    3: "PAD",
}
PAD_TOKEN_ID = 3


@dataclass
class DecoderProjection:
    """Projection layers used to convert hidden states to text logits."""

    out_norm: Optional[torch.nn.Module]
    text_linear: torch.nn.Module


class DecodeData:
    """Per-token layer-wise decode information.

    Args:
        attention_weights: Optional `[L, H, K_t]` tensor for one token.
        hidden_states: `[L, D]` hidden states for one token across layers.
        projection: Decoder projection used to convert hidden states to logits.
        tokenizer: Text tokenizer for token visualization.
    """

    def __init__(
        self,
        attention_weights: Optional[torch.Tensor],
        hidden_states: torch.Tensor,
        projection: DecoderProjection,
        tokenizer: sentencepiece.SentencePieceProcessor,
    ):
        self.attention_weights = attention_weights
        self.hidden_states = hidden_states.float()
        self._projection = projection
        self._tokenizer = tokenizer
        self.logits = self._calculate_logits()  # [L, V]
        self.token_ids = self._greedy_decode()  # [L]
        self.tokens = [self._id_to_piece(int(t)) for t in self.token_ids.tolist()]

    def _calculate_logits(self) -> torch.Tensor:
        """Project hidden states `[L, D]` to logits `[L, vocab_size]`.

        This mirrors the model decode head path used in `LMModel.forward_embeddings`:
        optional `out_norm` -> `text_linear`.
        The projection is executed in the same dtype/device as `text_linear.weight`
        for parity with model behavior, then converted to float32 for analysis.
        """
        x = self.hidden_states
        if x.dim() != 2:
            raise RuntimeError(f"Expected hidden states shape [L, D], got {tuple(x.shape)}")

        # `lm.out_norm` can be RMSNorm, which expects 3D `[B, T, D]`.
        text_linear_weight = self._projection.text_linear.weight
        proj_device = text_linear_weight.device
        proj_dtype = text_linear_weight.dtype

        x = x.to(device=proj_device, dtype=proj_dtype).unsqueeze(0)  # [1, L, D]
        with torch.no_grad():
            if self._projection.out_norm is not None:
                x = self._projection.out_norm(x)
            logits = self._projection.text_linear(x)
        logits = logits.squeeze(0)  # [L, V]
        if logits.dim() != 2:
            raise RuntimeError(f"Unexpected logits shape: {tuple(logits.shape)}")
        return logits.float().cpu()

    def _greedy_decode(self) -> torch.Tensor:
        """Greedy token ids for every layer: `[L]`."""
        return torch.argmax(self.logits, dim=-1)

    def get_final_output(self) -> tuple[int, str, torch.Tensor]:
        """Return final-layer token id, piece, and logits vector."""
        final_id = int(self.token_ids[-1].item())
        return final_id, self._id_to_piece(final_id), self.logits[-1]

    def jsd(self, layer_n: int) -> float:
        """Jensen-Shannon divergence vs final layer distribution."""
        if layer_n < 0 or layer_n >= self.logits.shape[0]:
            raise IndexError(f"layer_n={layer_n} out of range [0, {self.logits.shape[0] - 1}]")
        p = self.logits[layer_n]
        q = self.logits[-1]
        return float(_jsd_from_logits(p, q).item())

    def _id_to_piece(self, token_id: int) -> str:
        if token_id in SPECIAL_TOKEN_MAP:
            return SPECIAL_TOKEN_MAP[token_id]
        piece = self._tokenizer.id_to_piece(token_id)
        return piece.replace("‚ñÅ", " ") if piece is not None else f"<{token_id}>"


def _jsd_from_logits(logits_p: torch.Tensor, logits_q: torch.Tensor) -> torch.Tensor:
    p = F.softmax(logits_p.float(), dim=-1)
    q = F.softmax(logits_q.float(), dim=-1)
    m = 0.5 * (p + q)
    eps = torch.finfo(p.dtype).eps
    kl_pm = torch.sum(p * (torch.log(p + eps) - torch.log(m + eps)))
    kl_qm = torch.sum(q * (torch.log(q + eps) - torch.log(m + eps)))
    return 0.5 * (kl_pm + kl_qm)


def _resolve_token_range(t_total: int, start: int, end: int) -> tuple[int, int]:
    start_idx = max(0, start)
    end_idx = t_total if end < 0 else min(end, t_total)
    if start_idx >= end_idx:
        raise ValueError(f"Invalid range [{start_idx}, {end_idx}) for total tokens {t_total}")
    return start_idx, end_idx


def _load_input_transcript_spans(
    hidden_path: Path,
    frame_rate_hz: float,
    base: str = "input",
) -> list[tuple[float, float, str]]:
    """Load user transcript from sibling `input.json` and map time -> token index spans.

    Returns:
        List of `(start_token, end_token, text)` where token values are floats.
    """
    input_json = hidden_path.with_name(f"{base}.json")
    if not input_json.exists():
        return []

    with open(input_json, "r") as f:
        payload = json.load(f)

    chunks = payload.get("chunks", [])
    spans: list[tuple[float, float, str]] = []
    for chunk in chunks:
        text = str(chunk.get("text", "")).strip()
        ts = chunk.get("timestamp", None)
        if not text or not isinstance(ts, list) or len(ts) != 2:
            continue
        start_sec = float(ts[0])
        end_sec = float(ts[1])
        if end_sec <= start_sec:
            continue
        spans.append((start_sec * frame_rate_hz, end_sec * frame_rate_hz, text))
    return spans


def plot_logits_evolution(
    decode_data_list: list[DecodeData],
    output_path: str,
    token_start_idx: int = 0,
    transcript_spans: Optional[list[tuple[float, float, str]]] = None,
):
    """Plot layer-wise JSD heatmap and most likely token at each cell.

    Spec:
        - x-axis: token sequence index
        - y-axis: layer index (1..L)
        - cell color: JSD(layer, final-layer) where white=more similar, blue=less similar
        - text in each cell: greedy token for that layer and token position
    """
    if len(decode_data_list) == 0:
        raise ValueError("decode_data_list is empty")

    num_tokens = len(decode_data_list)
    num_layers = int(decode_data_list[0].logits.shape[0])

    jsd_grid = np.zeros((num_layers, num_tokens), dtype=np.float32)
    token_grid: list[list[str]] = [["" for _ in range(num_tokens)] for _ in range(num_layers)]

    for t, item in enumerate(decode_data_list):
        if item.logits.shape[0] != num_layers:
            raise ValueError("All DecodeData entries must have the same number of layers")
        for layer_idx in range(num_layers):
            jsd_grid[layer_idx, t] = item.jsd(layer_idx)
            token_grid[layer_idx][t] = item.tokens[layer_idx]

    fig_w = max(10.0, num_tokens * 0.7)
    fig_h = max(8.0, num_layers * 0.24)
    fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=150)

    positive_jsd = jsd_grid[jsd_grid > 0]
    if positive_jsd.size == 0:
        jsd_vmin = 1e-12
    else:
        jsd_vmin = max(float(np.min(positive_jsd)), 1e-12)

    im = ax.imshow(
        np.clip(jsd_grid, jsd_vmin, None),
        aspect="auto",
        cmap="Blues",
        origin="upper",
        norm=LogNorm(vmin=jsd_vmin, vmax=max(float(np.max(jsd_grid)), jsd_vmin)),
    )
    cbar = fig.colorbar(im, ax=ax)
    cbar.set_label("JSD vs final layer [log scale]")

    def _plot_safe_text(text: str) -> str:
        # Keep tokens visible while preventing mathtext parsing failures.
        return text.replace("\n", " ").replace("$", "\\$")

    for y in range(num_layers):
        for x in range(num_tokens):
            token_txt = _plot_safe_text(token_grid[y][x])
            try:
                ax.text(
                    x,
                    y,
                    token_txt,
                    ha="center",
                    va="center",
                    fontsize=6,
                    color="black",
                    parse_math=False,
                )
            except TypeError:
                # Older matplotlib without `parse_math` argument.
                ax.text(x, y, token_txt, ha="center", va="center", fontsize=6, color="black")

    ax.set_xlabel("Token index")
    ax.set_ylabel("Layer (1..L)")
    ax.set_xticks(np.arange(num_tokens))
    ax.set_yticks(np.arange(num_layers))
    ax.set_yticklabels([str(i + 1) for i in range(num_layers)])
    ax.set_title("Premature decode: layer-wise logits evolution")

    if transcript_spans:
        lane_y = float(num_layers)
        lane_h = 0.8
        for start_tok, end_tok, word in transcript_spans:
            local_start = start_tok - token_start_idx
            local_end = end_tok - token_start_idx
            if local_end <= -0.5 or local_start >= num_tokens - 0.5:
                continue
            draw_start = max(local_start, -0.5)
            draw_end = min(local_end, num_tokens - 0.5)
            if draw_end <= draw_start:
                continue
            rect = Rectangle(
                (draw_start, lane_y),
                draw_end - draw_start,
                lane_h,
                facecolor="#f3f3f3",
                edgecolor="#888888",
                linewidth=0.5,
                alpha=0.9,
            )
            ax.add_patch(rect)
            center_x = 0.5 * (draw_start + draw_end)
            safe_word = _plot_safe_text(word)
            try:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black", parse_math=False)
            except TypeError:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black")

        ax.axhline(num_layers - 0.5, color="#666666", linewidth=0.8)
        ax.text(-1.2, lane_y + lane_h / 2, "User", ha="right", va="center", fontsize=7, color="black")
        ax.set_ylim(num_layers + lane_h + 0.4, -0.5)

    fig.tight_layout()
    out_path = Path(output_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path)
    plt.close(fig)


def plot_final_token_probability_heatmap(
    decode_data_list: list[DecodeData],
    output_path: str,
    token_start_idx: int = 0,
    transcript_spans: Optional[list[tuple[float, float, str]]] = None,
):
    """Plot probability of the final-layer decoded token across all layers/steps.

    For each token step `t`, let `y_t` be the token id decoded at the final layer.
    This plot shows `P_layer_t(y_t)` for each layer at that same step.

    Heatmap layout:
        - x-axis: token step index
        - y-axis: layer index
        - cell value: probability in [0, 1]
    """
    if len(decode_data_list) == 0:
        raise ValueError("decode_data_list is empty")

    num_tokens = len(decode_data_list)
    num_layers = int(decode_data_list[0].logits.shape[0])
    prob_grid = np.zeros((num_layers, num_tokens), dtype=np.float32)
    token_grid: list[list[str]] = [["" for _ in range(num_tokens)] for _ in range(num_layers)]

    for t, item in enumerate(decode_data_list):
        if item.logits.shape[0] != num_layers:
            raise ValueError("All DecodeData entries must have the same number of layers")
        final_token_id = int(item.token_ids[-1].item())
        probs = F.softmax(item.logits.float(), dim=-1)  # [L, V]
        prob_grid[:, t] = probs[:, final_token_id].cpu().numpy()
        for layer_idx in range(num_layers):
            token_grid[layer_idx][t] = item.tokens[layer_idx]

    fig_w = max(10.0, num_tokens * 0.7)
    fig_h = max(8.0, num_layers * 0.24)
    fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=150)

    positive_probs = prob_grid[prob_grid > 0]
    if positive_probs.size == 0:
        prob_vmin = 1e-8
        prob_vmax = 1.0
    else:
        prob_vmin = max(float(np.min(positive_probs)), 1e-8)
        prob_vmax = max(float(np.max(positive_probs)), prob_vmin)

    im = ax.imshow(
        np.clip(prob_grid, prob_vmin, prob_vmax),
        aspect="auto",
        cmap="YlOrRd",
        origin="upper",
        norm=LogNorm(vmin=prob_vmin, vmax=prob_vmax),
    )
    cbar = fig.colorbar(im, ax=ax)
    cbar.set_label("P(final-layer decoded token) [log scale]")

    def _plot_safe_text(text: str) -> str:
        return text.replace("\n", " ").replace("$", "\\$")

    for y in range(num_layers):
        for x in range(num_tokens):
            token_txt = _plot_safe_text(token_grid[y][x])
            p = float(prob_grid[y, x])
            txt_color = "white" if p < 0.2 else "black"
            try:
                ax.text(
                    x,
                    y,
                    token_txt,
                    ha="center",
                    va="center",
                    fontsize=6,
                    color=txt_color,
                    parse_math=False,
                )
            except TypeError:
                ax.text(
                    x,
                    y,
                    token_txt,
                    ha="center",
                    va="center",
                    fontsize=6,
                    color=txt_color,
                )

    ax.set_xlabel("Token index")
    ax.set_ylabel("Layer (1..L)")
    ax.set_xticks(np.arange(num_tokens))
    ax.set_yticks(np.arange(num_layers))
    ax.set_yticklabels([str(i + 1) for i in range(num_layers)])
    ax.set_title("Premature decode: final-token probability across layers")

    if transcript_spans:
        lane_y = float(num_layers)
        lane_h = 0.8
        for start_tok, end_tok, word in transcript_spans:
            local_start = start_tok - token_start_idx
            local_end = end_tok - token_start_idx
            if local_end <= -0.5 or local_start >= num_tokens - 0.5:
                continue
            draw_start = max(local_start, -0.5)
            draw_end = min(local_end, num_tokens - 0.5)
            if draw_end <= draw_start:
                continue
            rect = Rectangle(
                (draw_start, lane_y),
                draw_end - draw_start,
                lane_h,
                facecolor="#f3f3f3",
                edgecolor="#888888",
                linewidth=0.5,
                alpha=0.9,
            )
            ax.add_patch(rect)
            center_x = 0.5 * (draw_start + draw_end)
            safe_word = _plot_safe_text(word)
            try:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black", parse_math=False)
            except TypeError:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black")

        ax.axhline(num_layers - 0.5, color="#666666", linewidth=0.8)
        ax.text(-1.2, lane_y + lane_h / 2, "User", ha="right", va="center", fontsize=7, color="black")
        ax.set_ylim(num_layers + lane_h + 0.4, -0.5)

    fig.tight_layout()
    out_path = Path(output_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path)
    plt.close(fig)


def plot_pad_token_probability_heatmap(
    decode_data_list: list[DecodeData],
    output_path: str,
    token_start_idx: int = 0,
    transcript_spans: Optional[list[tuple[float, float, str]]] = None,
):
    """Plot probability of `<PAD>` token (id=3) across all layers/steps."""
    if len(decode_data_list) == 0:
        raise ValueError("decode_data_list is empty")

    num_tokens = len(decode_data_list)
    num_layers = int(decode_data_list[0].logits.shape[0])
    prob_grid = np.zeros((num_layers, num_tokens), dtype=np.float32)
    token_grid: list[list[str]] = [["" for _ in range(num_tokens)] for _ in range(num_layers)]

    for t, item in enumerate(decode_data_list):
        if item.logits.shape[0] != num_layers:
            raise ValueError("All DecodeData entries must have the same number of layers")
        probs = F.softmax(item.logits.float(), dim=-1)  # [L, V]
        prob_grid[:, t] = probs[:, PAD_TOKEN_ID].cpu().numpy()
        for layer_idx in range(num_layers):
            token_grid[layer_idx][t] = item.tokens[layer_idx]

    fig_w = max(10.0, num_tokens * 0.7)
    fig_h = max(8.0, num_layers * 0.24)
    fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=150)

    positive_probs = prob_grid[prob_grid > 0]
    if positive_probs.size == 0:
        prob_vmin = 1e-8
        prob_vmax = 1.0
    else:
        prob_vmin = max(float(np.min(positive_probs)), 1e-8)
        prob_vmax = max(float(np.max(positive_probs)), prob_vmin)

    im = ax.imshow(
        np.clip(prob_grid, prob_vmin, prob_vmax),
        aspect="auto",
        cmap="YlOrRd",
        origin="upper",
        norm=LogNorm(vmin=prob_vmin, vmax=prob_vmax),
    )
    cbar = fig.colorbar(im, ax=ax)
    cbar.set_label("P(<PAD>) [log scale]")

    def _plot_safe_text(text: str) -> str:
        return text.replace("\n", " ").replace("$", "\\$")

    for y in range(num_layers):
        for x in range(num_tokens):
            token_txt = _plot_safe_text(token_grid[y][x])
            p = float(prob_grid[y, x])
            txt_color = "white" if p < 0.2 else "black"
            try:
                ax.text(
                    x,
                    y,
                    token_txt,
                    ha="center",
                    va="center",
                    fontsize=6,
                    color=txt_color,
                    parse_math=False,
                )
            except TypeError:
                ax.text(
                    x,
                    y,
                    token_txt,
                    ha="center",
                    va="center",
                    fontsize=6,
                    color=txt_color,
                )

    ax.set_xlabel("Token index")
    ax.set_ylabel("Layer (1..L)")
    ax.set_xticks(np.arange(num_tokens))
    ax.set_yticks(np.arange(num_layers))
    ax.set_yticklabels([str(i + 1) for i in range(num_layers)])
    ax.set_title("Premature decode: <PAD> probability across layers")

    if transcript_spans:
        lane_y = float(num_layers)
        lane_h = 0.8
        for start_tok, end_tok, word in transcript_spans:
            local_start = start_tok - token_start_idx
            local_end = end_tok - token_start_idx
            if local_end <= -0.5 or local_start >= num_tokens - 0.5:
                continue
            draw_start = max(local_start, -0.5)
            draw_end = min(local_end, num_tokens - 0.5)
            if draw_end <= draw_start:
                continue
            rect = Rectangle(
                (draw_start, lane_y),
                draw_end - draw_start,
                lane_h,
                facecolor="#f3f3f3",
                edgecolor="#888888",
                linewidth=0.5,
                alpha=0.9,
            )
            ax.add_patch(rect)
            center_x = 0.5 * (draw_start + draw_end)
            safe_word = _plot_safe_text(word)
            try:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black", parse_math=False)
            except TypeError:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black")

        ax.axhline(num_layers - 0.5, color="#666666", linewidth=0.8)
        ax.text(-1.2, lane_y + lane_h / 2, "User", ha="right", va="center", fontsize=7, color="black")
        ax.set_ylim(num_layers + lane_h + 0.4, -0.5)

    fig.tight_layout()
    out_path = Path(output_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path)
    plt.close(fig)


def decode_preview(token_names: Sequence[str], chunk: int = 10) -> str:
    """Return a compact preview string for final output tokens.

    Uses `.` for `PAD` / `EPAD`.
    """
    clean = ["." if t in {"PAD", "EPAD"} else t for t in token_names]
    lines = []
    for i in range(0, len(clean), chunk):
        tokens = " ".join(clean[i : i + chunk])
        lines.append(f"{i:<4d} {tokens}")
    return "\n".join(lines)


def _resolve_hidden_paths(root_dir: Path, sample_number: int) -> list[Path]:
    """Return all ``*_hidden.pt`` files under ``root_dir/<sample_number>/``."""
    sample_dir = root_dir / str(sample_number)
    if not sample_dir.is_dir():
        raise FileNotFoundError(f"Sample directory not found: {sample_dir}")
    pts = sorted(sample_dir.glob("*_hidden.pt"))
    if not pts:
        raise FileNotFoundError(f"No *_hidden.pt files found in {sample_dir}")
    return pts


def _load_decoder_projection(
    *,
    hf_repo: str,
    moshi_weight: Optional[str],
    device: str,
) -> DecoderProjection:
    if moshi_weight is None:
        moshi_weight = hf_hub_download(hf_repo, loaders.MOSHI_NAME)  # type: ignore
    lm = loaders.get_moshi_lm(moshi_weight, device=device, cpu_offload=(device == "cpu"))
    lm.eval()
    return DecoderProjection(out_norm=lm.out_norm, text_linear=lm.text_linear)


def _load_tokenizer(hf_repo: str, tokenizer_path: Optional[str]) -> sentencepiece.SentencePieceProcessor:
    if tokenizer_path is None:
        tokenizer_path = hf_hub_download(hf_repo, loaders.TEXT_TOKENIZER_NAME)  # type: ignore
    return sentencepiece.SentencePieceProcessor(tokenizer_path)  # type: ignore


def premature_decode(
    hidden_payload: dict,
    projection: DecoderProjection,
    tokenizer: sentencepiece.SentencePieceProcessor,
    start: int,
    end: int,
) -> list[DecodeData]:
    """Build DecodeData list for token range [start, end)."""
    hidden = hidden_payload["text_hidden_layers"]
    if not isinstance(hidden, torch.Tensor) or hidden.dim() != 3:
        raise ValueError("Expected payload['text_hidden_layers'] with shape [T, L, D]")

    t_total = int(hidden.shape[0])
    start, end = _resolve_token_range(t_total, start, end)

    attention_list = hidden_payload.get("text_attention_weights", [None] * t_total)
    if len(attention_list) != t_total:
        attention_list = [None] * t_total

    decode_data_list: list[DecodeData] = []
    for t in range(start, end):
        decode_data_list.append(
            DecodeData(
                attention_weights=attention_list[t],
                hidden_states=hidden[t],
                projection=projection,
                tokenizer=tokenizer,
            )
        )
    return decode_data_list

def plot_pad_lookback_ratio(
    decode_data_list: list[DecodeData],
    output_path: str,
    input_wav: str,
    token_offset: int = 0,
    token_start_idx: int = 0,
    transcript_spans: Optional[list[tuple[float, float, str]]] = None,
):
    """Import attention_pad.py to plot the ratio of attention to PAD tokens across layers and steps.
    use the same layout as the other heatmaps, but with cell value = that ratio.
    """
    from .attention_pad import pad_lookback_ratio

    if len(decode_data_list) == 0:
        raise ValueError("decode_data_list is empty")

    print("Calculating PAD lookback ratio...")
    ratio = pad_lookback_ratio(
        decode_data_list, input_wav=input_wav, token_offset=token_offset,
    )  # [T, L]
    if ratio.numel() == 0:
        raise ValueError("pad_lookback_ratio returned empty tensor")

    ratio_grid = ratio.transpose(0, 1).cpu().numpy().astype(np.float32)  # [L, T]
    num_layers, num_tokens = ratio_grid.shape

    token_grid: list[list[str]] = [["" for _ in range(num_tokens)] for _ in range(num_layers)]
    for t, item in enumerate(decode_data_list):
        for layer_idx in range(num_layers):
            token_grid[layer_idx][t] = item.tokens[layer_idx]

    fig_w = max(10.0, num_tokens * 0.7)
    fig_h = max(8.0, num_layers * 0.24)
    fig, ax = plt.subplots(figsize=(fig_w, fig_h), dpi=150)

    positive_ratio = ratio_grid[ratio_grid > 0]
    if positive_ratio.size == 0:
        ratio_vmin = 1e-8
        ratio_vmax = 1.0
    else:
        ratio_vmin = max(float(np.min(positive_ratio)), 1e-8)
        ratio_vmax = max(float(np.max(positive_ratio)), ratio_vmin)

    im = ax.imshow(
        np.clip(ratio_grid, ratio_vmin, ratio_vmax),
        aspect="auto",
        cmap="PuBuGn",
        origin="upper",
        norm=LogNorm(vmin=ratio_vmin, vmax=ratio_vmax),
    )
    cbar = fig.colorbar(im, ax=ax)
    cbar.set_label("PAD lookback ratio [log scale]")

    def _plot_safe_text(text: str) -> str:
        return text.replace("\n", " ").replace("$", "\\$")

    for y in range(num_layers):
        for x in range(num_tokens):
            token_txt = _plot_safe_text(token_grid[y][x])
            v = float(ratio_grid[y, x])
            txt_color = "white" if v > 0.45 else "black"
            try:
                ax.text(
                    x,
                    y,
                    token_txt,
                    ha="center",
                    va="center",
                    fontsize=6,
                    color=txt_color,
                    parse_math=False,
                )
            except TypeError:
                ax.text(x, y, token_txt, ha="center", va="center", fontsize=6, color=txt_color)

    ax.set_xlabel("Token index")
    ax.set_ylabel("Layer (1..L)")
    ax.set_xticks(np.arange(num_tokens))
    ax.set_yticks(np.arange(num_layers))
    ax.set_yticklabels([str(i + 1) for i in range(num_layers)])
    ax.set_title("Premature decode: PAD lookback ratio")

    if transcript_spans:
        lane_y = float(num_layers)
        lane_h = 0.8
        for start_tok, end_tok, word in transcript_spans:
            local_start = start_tok - token_start_idx
            local_end = end_tok - token_start_idx
            if local_end <= -0.5 or local_start >= num_tokens - 0.5:
                continue
            draw_start = max(local_start, -0.5)
            draw_end = min(local_end, num_tokens - 0.5)
            if draw_end <= draw_start:
                continue
            rect = Rectangle(
                (draw_start, lane_y),
                draw_end - draw_start,
                lane_h,
                facecolor="#f3f3f3",
                edgecolor="#888888",
                linewidth=0.5,
                alpha=0.9,
            )
            ax.add_patch(rect)
            center_x = 0.5 * (draw_start + draw_end)
            safe_word = _plot_safe_text(word)
            try:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black", parse_math=False)
            except TypeError:
                ax.text(center_x, lane_y + lane_h / 2, safe_word, ha="center", va="center", fontsize=6, color="black")

        ax.axhline(num_layers - 0.5, color="#666666", linewidth=0.8)
        ax.text(-1.2, lane_y + lane_h / 2, "User", ha="right", va="center", fontsize=7, color="black")
        ax.set_ylim(num_layers + lane_h + 0.4, -0.5)

    fig.tight_layout()
    out_path = Path(output_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(out_path)
    plt.close(fig)

def main():
    import logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(name)s  %(levelname)s  %(message)s",
    )
    ap = argparse.ArgumentParser("premature_decode")
    ap.add_argument("--root-dir", type=str, required=True, help="Root dir containing <n>/output_hidden.pt")
    ap.add_argument("-n", "--sample-number", type=int, required=True, help="Sample folder number under root-dir")
    ap.add_argument("--hidden-path", type=str, default=None, help="Direct path to output_hidden.pt (overrides root-dir/-n)")
    ap.add_argument("-s", "--start", type=int, default=0, help="Start token index (inclusive)")
    ap.add_argument("-e", "--end", type=int, default=-1, help="End token index (exclusive), -1 means end")
    ap.add_argument("--output", type=str, default=None, help="Output figure path")
    ap.add_argument("--preview-output", action="store_true", help="Print final decoded output preview")
    ap.add_argument("--hf-repo", type=str, default=loaders.DEFAULT_REPO)
    ap.add_argument("--tokenizer", type=str, default=None)
    ap.add_argument("--moshi-weight", type=str, default=None)
    ap.add_argument("--device", type=str, default="cpu")
    ap.add_argument("--transcript-base", type=str, default="auto", help="Base name for user transcript JSON. 'auto' (default) derives from hidden stem by stripping '_hidden', e.g. complete_sentence_hidden.pt -> complete_sentence.json. Use 'input' for sibling input.json.")
    args = ap.parse_args()

    root_dir = Path(args.root_dir)
    if args.hidden_path:
        hidden_paths = [Path(args.hidden_path)]
    else:
        hidden_paths = _resolve_hidden_paths(root_dir, args.sample_number)

    # Preview mode: just show token names from the first valid payload.
    if args.preview_output:
        for hp in hidden_paths:
            payload = torch.load(hp, map_location="cpu", weights_only=True)
            token_names = payload.get("token_names", [])
            if token_names:
                print(f"--- {hp.name} ---")
                print(decode_preview(token_names))
        return

    print("Loading decoder projection and tokenizer...")
    tokenizer = _load_tokenizer(args.hf_repo, args.tokenizer)
    projection = _load_decoder_projection(
        hf_repo=args.hf_repo,
        moshi_weight=args.moshi_weight,
        device=args.device,
    )

    for hidden_path in hidden_paths:
        if not hidden_path.exists():
            print(f"[SKIP] Hidden payload not found: {hidden_path}")
            continue

        try:
            payload = torch.load(hidden_path, map_location="cpu", weights_only=True)
        except (EOFError, RuntimeError) as exc:
            print(f"[SKIP] {hidden_path.name}: failed to load ({exc.__class__.__name__}: {exc})")
            continue
        if "text_hidden_layers" not in payload:
            print(f"[SKIP] {hidden_path.name}: missing key 'text_hidden_layers'")
            continue

        stem = hidden_path.stem  # e.g. "output_hidden"
        print(f"\n=== Processing {hidden_path} ===")

        t_total = int(payload["text_hidden_layers"].shape[0])
        start_idx, end_idx = _resolve_token_range(t_total, args.start, args.end)
        frame_rate_hz = float(payload.get("frame_rate", 12.5))

        # Derive transcript base: 'auto' strips '_hidden' from stem.
        if args.transcript_base == "auto":
            t_base = stem.removesuffix("_hidden") if stem.endswith("_hidden") else "input"
        else:
            t_base = args.transcript_base
        transcript_spans = _load_input_transcript_spans(hidden_path, frame_rate_hz, base=t_base)
        print(f"[DEBUG] stem={stem!r}, transcript_base={t_base!r}, spans={len(transcript_spans)}")

        print("Running premature decode...")
        decode_data = premature_decode(
            hidden_payload=payload,
            projection=projection,
            tokenizer=tokenizer,
            start=start_idx,
            end=end_idx,
        )

        if args.output and len(hidden_paths) == 1:
            output_path = Path(args.output)
        else:
            output_path = hidden_path.with_name(f"{stem}_logits_evolution_{start_idx}_{end_idx}.png")

        print("Plotting logits evolution heatmap...")
        plot_logits_evolution(
            decode_data,
            str(output_path),
            token_start_idx=start_idx,
            transcript_spans=transcript_spans,
        )

        if args.output and len(hidden_paths) == 1:
            prob_output_path = output_path.with_name(f"{output_path.stem}_final_token_prob{output_path.suffix}")
        else:
            prob_output_path = hidden_path.with_name(f"{stem}_final_token_probability_{start_idx}_{end_idx}.png")

        print("Plotting final token probability heatmap...")
        plot_final_token_probability_heatmap(
            decode_data,
            str(prob_output_path),
            token_start_idx=start_idx,
            transcript_spans=transcript_spans,
        )

        if args.output and len(hidden_paths) == 1:
            pad_prob_output_path = output_path.with_name(f"{output_path.stem}_pad_token_prob{output_path.suffix}")
        else:
            pad_prob_output_path = hidden_path.with_name(f"{stem}_pad_token_probability_{start_idx}_{end_idx}.png")

        print("Plotting <PAD> token probability heatmap...")
        plot_pad_token_probability_heatmap(
            decode_data,
            str(pad_prob_output_path),
            token_start_idx=start_idx,
            transcript_spans=transcript_spans,
        )

        if args.output and len(hidden_paths) == 1:
            pad_lookback_output_path = output_path.with_name(f"{output_path.stem}_pad_lookback_ratio{output_path.suffix}")
        else:
            pad_lookback_output_path = hidden_path.with_name(f"{stem}_pad_lookback_ratio_{start_idx}_{end_idx}.png")

        input_wav_path = hidden_path.with_name("input.wav")
        if not input_wav_path.exists():
            print(f"[SKIP] Input wav not found for PAD lookback ratio: {input_wav_path}")
        else:
            print("Plotting PAD lookback ratio heatmap...")
            plot_pad_lookback_ratio(
                decode_data,
                str(pad_lookback_output_path),
                input_wav=str(input_wav_path),
                token_offset=start_idx,
                token_start_idx=start_idx,
                transcript_spans=transcript_spans,
            )
            print(f"Saved figure to {pad_lookback_output_path}")

        print(f"Saved figure to {output_path}")
        print(f"Saved figure to {prob_output_path}")
        print(f"Saved figure to {pad_prob_output_path}")


if __name__ == "__main__":
    main()
